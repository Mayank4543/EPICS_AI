{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "590d6350",
   "metadata": {},
   "source": [
    "# Hand Gesture Recognition Flask API\n",
    "\n",
    "## Complete Implementation Guide\n",
    "\n",
    "This notebook provides a comprehensive Flask backend implementation for hand gesture recognition using MediaPipe Hands and scikit-learn. The system allows you to:\n",
    "\n",
    "1. **Record Gestures**: Capture hand landmarks from base64 images\n",
    "2. **Train Model**: Use RandomForest classifier on recorded data\n",
    "3. **Detect Gestures**: Predict gestures from new images\n",
    "\n",
    "### Key Features:\n",
    "- ✅ MediaPipe Hands for 21-point hand landmark detection\n",
    "- ✅ CSV data storage with structured format\n",
    "- ✅ RandomForest machine learning classifier\n",
    "- ✅ RESTful API endpoints with error handling\n",
    "- ✅ CORS support for frontend integration\n",
    "- ✅ Clean, production-ready code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d20faf3",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the necessary libraries for our gesture recognition system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833addc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Flask Backend Server for Hand Gesture Recognition\n",
    "Uses MediaPipe Hands for real-time hand landmark detection from base64 images.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from PIL import Image\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb43c8",
   "metadata": {},
   "source": [
    "## 2. Flask Application Setup\n",
    "\n",
    "Configure the Flask application with CORS support, logging, and basic settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d089b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "CORS(app)  # Enable CORS for all routes\n",
    "\n",
    "# Configure Flask\n",
    "app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024  # 50MB max file size\n",
    "\n",
    "# Data storage configuration\n",
    "CSV_FILE_PATH = os.path.join(os.path.dirname(__file__), 'gesture_data.csv')\n",
    "MODEL_FILE_PATH = os.path.join(os.path.dirname(__file__), 'gesture_model.pkl')\n",
    "\n",
    "print(\"✅ Flask application configured successfully!\")\n",
    "print(f\"📁 CSV File Path: {CSV_FILE_PATH}\")\n",
    "print(f\"🤖 Model File Path: {MODEL_FILE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc64e9b",
   "metadata": {},
   "source": [
    "## 3. MediaPipe Model Initialization\n",
    "\n",
    "Initialize the MediaPipe Hands model for hand landmark detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486cf638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Global MediaPipe Hands model\n",
    "hands_model = None\n",
    "\n",
    "def initialize_mediapipe_model():\n",
    "    \"\"\"Initialize the MediaPipe Hands model with optimal settings.\"\"\"\n",
    "    global hands_model\n",
    "    try:\n",
    "        logger.info(\"Initializing MediaPipe Hands model...\")\n",
    "        hands_model = mp_hands.Hands(\n",
    "            static_image_mode=True,\n",
    "            max_num_hands=2,\n",
    "            min_detection_confidence=0.7,\n",
    "            min_tracking_confidence=0.5\n",
    "        )\n",
    "        logger.info(\"✅ MediaPipe Hands model initialized successfully!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Failed to initialize MediaPipe Hands model: {e}\")\n",
    "        return False\n",
    "\n",
    "# Initialize the model\n",
    "if initialize_mediapipe_model():\n",
    "    print(\"✅ MediaPipe model ready for use!\")\n",
    "else:\n",
    "    print(\"❌ Failed to initialize MediaPipe model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9dce1b",
   "metadata": {},
   "source": [
    "## 4. Base64 Image Processing Functions\n",
    "\n",
    "Functions to handle base64 image decoding and preprocessing for MediaPipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0a1f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_base64_image(base64_string: str) -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Decode base64 image string to OpenCV image format.\n",
    "    \n",
    "    Args:\n",
    "        base64_string: Base64 encoded image string (with or without data URL prefix)\n",
    "        \n",
    "    Returns:\n",
    "        OpenCV image array or None if decoding fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove data URL prefix if present\n",
    "        if base64_string.startswith('data:image/'):\n",
    "            base64_string = base64_string.split(',')[1]\n",
    "        \n",
    "        # Decode base64 to bytes\n",
    "        image_bytes = base64.b64decode(base64_string)\n",
    "        \n",
    "        # Convert bytes to PIL Image\n",
    "        pil_image = Image.open(io.BytesIO(image_bytes))\n",
    "        \n",
    "        # Convert PIL to OpenCV format (RGB to BGR)\n",
    "        opencv_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        return opencv_image\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to decode base64 image: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"✅ Base64 image processing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ac777f",
   "metadata": {},
   "source": [
    "## 5. Hand Landmark Processing Functions\n",
    "\n",
    "Functions to extract and process hand landmarks from MediaPipe results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b129c961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landmark_name(index: int) -> str:\n",
    "    \"\"\"\n",
    "    Get the name of a hand landmark by its index.\n",
    "    \n",
    "    Args:\n",
    "        index: Landmark index (0-20)\n",
    "        \n",
    "    Returns:\n",
    "        Human-readable landmark name\n",
    "    \"\"\"\n",
    "    landmark_names = [\n",
    "        'WRIST',\n",
    "        'THUMB_CMC', 'THUMB_MCP', 'THUMB_IP', 'THUMB_TIP',\n",
    "        'INDEX_FINGER_MCP', 'INDEX_FINGER_PIP', 'INDEX_FINGER_DIP', 'INDEX_FINGER_TIP',\n",
    "        'MIDDLE_FINGER_MCP', 'MIDDLE_FINGER_PIP', 'MIDDLE_FINGER_DIP', 'MIDDLE_FINGER_TIP',\n",
    "        'RING_FINGER_MCP', 'RING_FINGER_PIP', 'RING_FINGER_DIP', 'RING_FINGER_TIP',\n",
    "        'PINKY_MCP', 'PINKY_PIP', 'PINKY_DIP', 'PINKY_TIP'\n",
    "    ]\n",
    "    \n",
    "    return landmark_names[index] if index < len(landmark_names) else f'LANDMARK_{index}'\n",
    "\n",
    "def process_hand_landmarks(results) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process MediaPipe hand landmarks results into structured format.\n",
    "    \n",
    "    Args:\n",
    "        results: MediaPipe Hands results object\n",
    "        \n",
    "    Returns:\n",
    "        List of hand detection results with landmarks and metadata\n",
    "    \"\"\"\n",
    "    if not results.multi_hand_landmarks:\n",
    "        return []\n",
    "    \n",
    "    processed_hands = []\n",
    "    \n",
    "    for hand_idx, (hand_landmarks, handedness) in enumerate(\n",
    "        zip(results.multi_hand_landmarks, results.multi_handedness)\n",
    "    ):\n",
    "        # Extract landmark coordinates\n",
    "        landmarks = []\n",
    "        for idx, landmark in enumerate(hand_landmarks.landmark):\n",
    "            landmarks.append({\n",
    "                'index': idx,\n",
    "                'name': get_landmark_name(idx),\n",
    "                'x': round(landmark.x, 4),\n",
    "                'y': round(landmark.y, 4),\n",
    "                'z': round(landmark.z, 4),\n",
    "                'visibility': round(landmark.visibility, 4) if hasattr(landmark, 'visibility') else 1.0\n",
    "            })\n",
    "        \n",
    "        # Get handedness information\n",
    "        hand_label = handedness.classification[0].label\n",
    "        hand_confidence = handedness.classification[0].score\n",
    "        \n",
    "        processed_hands.append({\n",
    "            'handIndex': hand_idx,\n",
    "            'handedness': hand_label.lower(),  # 'left' or 'right'\n",
    "            'confidence': round(hand_confidence, 4),\n",
    "            'landmarks': landmarks,\n",
    "            'totalLandmarks': len(landmarks)\n",
    "        })\n",
    "    \n",
    "    return processed_hands\n",
    "\n",
    "print(\"✅ Hand landmark processing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742c7534",
   "metadata": {},
   "source": [
    "## 6. CSV Data Management Functions\n",
    "\n",
    "Functions to manage gesture data storage in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3dca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_csv_file():\n",
    "    \"\"\"Initialize CSV file with headers if it doesn't exist.\"\"\"\n",
    "    if not os.path.exists(CSV_FILE_PATH):\n",
    "        headers = ['gesture_name']\n",
    "        # Add coordinate columns for 21 landmarks (x0,y0,z0, x1,y1,z1, ..., x20,y20,z20)\n",
    "        for i in range(21):\n",
    "            headers.extend([f'x{i}', f'y{i}', f'z{i}'])\n",
    "        \n",
    "        df = pd.DataFrame(columns=headers)\n",
    "        df.to_csv(CSV_FILE_PATH, index=False)\n",
    "        logger.info(f\"✅ Created CSV file: {CSV_FILE_PATH}\")\n",
    "\n",
    "def save_landmarks_to_csv(gesture_name: str, landmarks: List[Dict[str, Any]]) -> bool:\n",
    "    \"\"\"\n",
    "    Save hand landmarks to CSV file.\n",
    "    \n",
    "    Args:\n",
    "        gesture_name: Name of the gesture\n",
    "        landmarks: List of landmark dictionaries from process_hand_landmarks\n",
    "        \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract coordinates from landmarks (assuming first hand if multiple detected)\n",
    "        if not landmarks:\n",
    "            logger.error(\"No landmarks provided to save\")\n",
    "            return False\n",
    "        \n",
    "        first_hand = landmarks[0]['landmarks']\n",
    "        if len(first_hand) != 21:\n",
    "            logger.error(f\"Expected 21 landmarks, got {len(first_hand)}\")\n",
    "            return False\n",
    "        \n",
    "        # Create data row\n",
    "        row_data = {'gesture_name': gesture_name}\n",
    "        for i, landmark in enumerate(first_hand):\n",
    "            row_data[f'x{i}'] = landmark['x']\n",
    "            row_data[f'y{i}'] = landmark['y']\n",
    "            row_data[f'z{i}'] = landmark['z']\n",
    "        \n",
    "        # Append to CSV\n",
    "        df = pd.DataFrame([row_data])\n",
    "        if os.path.exists(CSV_FILE_PATH):\n",
    "            df.to_csv(CSV_FILE_PATH, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            initialize_csv_file()\n",
    "            df.to_csv(CSV_FILE_PATH, mode='a', header=False, index=False)\n",
    "        \n",
    "        logger.info(f\"✅ Saved landmarks for gesture '{gesture_name}' to CSV\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save landmarks to CSV: {e}\")\n",
    "        return False\n",
    "\n",
    "# Initialize CSV file\n",
    "initialize_csv_file()\n",
    "print(\"✅ CSV data management functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a144d1",
   "metadata": {},
   "source": [
    "## 7. Machine Learning Model Functions\n",
    "\n",
    "Functions to train and manage the RandomForest classifier for gesture recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model() -> Optional[RandomForestClassifier]:\n",
    "    \"\"\"Load the trained gesture recognition model.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(MODEL_FILE_PATH):\n",
    "            model = joblib.load(MODEL_FILE_PATH)\n",
    "            logger.info(\"✅ Loaded trained model\")\n",
    "            return model\n",
    "        else:\n",
    "            logger.warning(\"No trained model found\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load model: {e}\")\n",
    "        return None\n",
    "\n",
    "def train_gesture_model() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Train a RandomForest classifier on the gesture data.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with training results and metrics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(CSV_FILE_PATH):\n",
    "            return {\n",
    "                'success': False,\n",
    "                'message': 'No training data found. Please record some gestures first.'\n",
    "            }\n",
    "        \n",
    "        # Load data\n",
    "        df = pd.read_csv(CSV_FILE_PATH)\n",
    "        if len(df) == 0:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'message': 'CSV file is empty. Please record some gestures first.'\n",
    "            }\n",
    "        \n",
    "        # Check if we have enough data\n",
    "        gesture_counts = df['gesture_name'].value_counts()\n",
    "        if len(gesture_counts) < 2:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'message': 'Need at least 2 different gestures to train a model.'\n",
    "            }\n",
    "        \n",
    "        min_samples = gesture_counts.min()\n",
    "        if min_samples < 3:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'message': f'Each gesture needs at least 3 samples. Found minimum: {min_samples}'\n",
    "            }\n",
    "        \n",
    "        # Prepare features and labels\n",
    "        X = df.drop('gesture_name', axis=1).values\n",
    "        y = df['gesture_name'].values\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate model\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Save model\n",
    "        joblib.dump(model, MODEL_FILE_PATH)\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'message': 'Model trained successfully',\n",
    "            'accuracy': round(accuracy, 4),\n",
    "            'totalSamples': len(df),\n",
    "            'gestureCount': len(gesture_counts),\n",
    "            'gestures': gesture_counts.to_dict(),\n",
    "            'trainingSamples': len(X_train),\n",
    "            'testingSamples': len(X_test)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training failed: {e}\")\n",
    "        return {\n",
    "            'success': False,\n",
    "            'message': f'Training failed: {str(e)}'\n",
    "        }\n",
    "\n",
    "print(\"✅ Machine learning model functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e849486",
   "metadata": {},
   "source": [
    "## 8. Flask API Endpoints\n",
    "\n",
    "Implementation of all required API endpoints for gesture recognition system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663092d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    \"\"\"Health check endpoint.\"\"\"\n",
    "    return jsonify({\n",
    "        'status': 'OK',\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'modelLoaded': hands_model is not None,\n",
    "        'service': 'Hand Gesture Recognition API',\n",
    "        'version': '1.0.0'\n",
    "    })\n",
    "\n",
    "@app.route('/model-info', methods=['GET'])\n",
    "def model_info():\n",
    "    \"\"\"Get information about the loaded model.\"\"\"\n",
    "    return jsonify({\n",
    "        'modelLoaded': hands_model is not None,\n",
    "        'modelType': 'MediaPipe Hands',\n",
    "        'version': mp.__version__,\n",
    "        'description': 'MediaPipe Hands for real-time hand landmark detection',\n",
    "        'maxHands': 2,\n",
    "        'landmarks': 21,\n",
    "        'inputFormat': 'base64-encoded image (data:image/...)',\n",
    "        'maxImageSize': '50MB',\n",
    "        'detectionConfidence': 0.7,\n",
    "        'trackingConfidence': 0.5\n",
    "    })\n",
    "\n",
    "@app.route('/record-gesture', methods=['POST'])\n",
    "def record_gesture():\n",
    "    \"\"\"\n",
    "    Record a gesture by extracting landmarks and saving to CSV.\n",
    "    Accepts gesture_name and base64 image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if model is loaded\n",
    "        if hands_model is None:\n",
    "            return jsonify({\n",
    "                'error': 'Model not loaded',\n",
    "                'message': 'MediaPipe Hands model is not initialized. Please try again later.'\n",
    "            }), 503\n",
    "        \n",
    "        # Validate request\n",
    "        if not request.is_json:\n",
    "            return jsonify({\n",
    "                'error': 'Invalid content type',\n",
    "                'message': 'Request must be JSON with application/json content type.'\n",
    "            }), 400\n",
    "        \n",
    "        data = request.get_json()\n",
    "        if not data or 'gesture_name' not in data or 'image' not in data:\n",
    "            return jsonify({\n",
    "                'error': 'Missing required data',\n",
    "                'message': 'Please provide both gesture_name and base64-encoded image in the request body.'\n",
    "            }), 400\n",
    "        \n",
    "        gesture_name = data['gesture_name']\n",
    "        base64_image = data['image']\n",
    "        \n",
    "        # Validate inputs\n",
    "        if not isinstance(gesture_name, str) or not gesture_name.strip():\n",
    "            return jsonify({\n",
    "                'error': 'Invalid gesture name',\n",
    "                'message': 'Gesture name must be a non-empty string.'\n",
    "            }), 400\n",
    "        \n",
    "        if not isinstance(base64_image, str):\n",
    "            return jsonify({\n",
    "                'error': 'Invalid image format',\n",
    "                'message': 'Image must be a base64-encoded string.'\n",
    "            }), 400\n",
    "        \n",
    "        gesture_name = gesture_name.strip()\n",
    "        logger.info(f\"Recording gesture: {gesture_name}\")\n",
    "        \n",
    "        # Decode base64 image\n",
    "        image = decode_base64_image(base64_image)\n",
    "        if image is None:\n",
    "            return jsonify({\n",
    "                'error': 'Image decoding failed',\n",
    "                'message': \"Unable to decode the provided base64 image. Please ensure it's a valid image format.\"\n",
    "            }), 400\n",
    "        \n",
    "        # Convert BGR to RGB for MediaPipe\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Run hand detection\n",
    "        try:\n",
    "            logger.info(\"Running MediaPipe hand detection for recording...\")\n",
    "            results = hands_model.process(rgb_image)\n",
    "            logger.info(f\"Detection complete. Found {len(results.multi_hand_landmarks) if results.multi_hand_landmarks else 0} hand(s)\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"MediaPipe detection error: {e}\")\n",
    "            return jsonify({\n",
    "                'error': 'Detection failed',\n",
    "                'message': 'An error occurred during hand detection processing.'\n",
    "            }), 500\n",
    "        \n",
    "        # Check if hands were detected\n",
    "        if not results.multi_hand_landmarks:\n",
    "            return jsonify({\n",
    "                'success': False,\n",
    "                'message': 'No hands detected in the image. Please ensure your hand is clearly visible.',\n",
    "                'gesture_name': gesture_name,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }), 400\n",
    "        \n",
    "        # Process landmarks\n",
    "        processed_hands = process_hand_landmarks(results)\n",
    "        \n",
    "        # Save to CSV\n",
    "        if save_landmarks_to_csv(gesture_name, processed_hands):\n",
    "            # Get updated dataset info\n",
    "            dataset_info = {}\n",
    "            if os.path.exists(CSV_FILE_PATH):\n",
    "                df = pd.read_csv(CSV_FILE_PATH)\n",
    "                dataset_info = {\n",
    "                    'totalSamples': len(df),\n",
    "                    'gestures': df['gesture_name'].value_counts().to_dict()\n",
    "                }\n",
    "            \n",
    "            return jsonify({\n",
    "                'success': True,\n",
    "                'message': f'Gesture \"{gesture_name}\" recorded successfully',\n",
    "                'gesture_name': gesture_name,\n",
    "                'handsDetected': len(processed_hands),\n",
    "                'landmarksSaved': True,\n",
    "                'dataset': dataset_info,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "        else:\n",
    "            return jsonify({\n",
    "                'error': 'Failed to save data',\n",
    "                'message': 'Could not save landmark data to CSV file.',\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }), 500\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error in record_gesture: {e}\")\n",
    "        return jsonify({\n",
    "            'error': 'Internal server error',\n",
    "            'message': 'An unexpected error occurred while recording the gesture.',\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }), 500\n",
    "\n",
    "print(\"✅ API endpoints part 1 defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be091f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/train', methods=['POST'])\n",
    "def train_model_endpoint():\n",
    "    \"\"\"\n",
    "    Train the gesture recognition model using recorded data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting model training...\")\n",
    "        \n",
    "        # Initialize CSV if it doesn't exist\n",
    "        initialize_csv_file()\n",
    "        \n",
    "        # Train the model\n",
    "        result = train_gesture_model()\n",
    "        \n",
    "        if result['success']:\n",
    "            logger.info(\"✅ Model training completed successfully\")\n",
    "            return jsonify({\n",
    "                **result,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'modelPath': MODEL_FILE_PATH\n",
    "            })\n",
    "        else:\n",
    "            logger.warning(f\"Training failed: {result['message']}\")\n",
    "            return jsonify({\n",
    "                **result,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            }), 400\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error in train_model: {e}\")\n",
    "        return jsonify({\n",
    "            'error': 'Internal server error',\n",
    "            'message': 'An unexpected error occurred during model training.',\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }), 500\n",
    "\n",
    "@app.route('/detect', methods=['POST'])\n",
    "def detect_trained_gesture():\n",
    "    \"\"\"\n",
    "    Detect gesture using the trained model.\n",
    "    Accepts base64 image and returns predicted gesture name.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if MediaPipe model is loaded\n",
    "        if hands_model is None:\n",
    "            return jsonify({\n",
    "                'error': 'MediaPipe model not loaded',\n",
    "                'message': 'MediaPipe Hands model is not initialized. Please try again later.'\n",
    "            }), 503\n",
    "        \n",
    "        # Load trained model\n",
    "        model = load_model()\n",
    "        if model is None:\n",
    "            return jsonify({\n",
    "                'error': 'No trained model',\n",
    "                'message': 'No trained model found. Please train a model first using the /train endpoint.'\n",
    "            }), 404\n",
    "        \n",
    "        # Validate request\n",
    "        if not request.is_json:\n",
    "            return jsonify({\n",
    "                'error': 'Invalid content type',\n",
    "                'message': 'Request must be JSON with application/json content type.'\n",
    "            }), 400\n",
    "        \n",
    "        data = request.get_json()\n",
    "        if not data or 'image' not in data:\n",
    "            return jsonify({\n",
    "                'error': 'Missing image data',\n",
    "                'message': 'Please provide a base64-encoded image in the request body.'\n",
    "            }), 400\n",
    "        \n",
    "        base64_image = data['image']\n",
    "        \n",
    "        # Validate base64 format\n",
    "        if not isinstance(base64_image, str):\n",
    "            return jsonify({\n",
    "                'error': 'Invalid image format',\n",
    "                'message': 'Image must be a base64-encoded string.'\n",
    "            }), 400\n",
    "        \n",
    "        logger.info(\"Processing gesture detection with trained model...\")\n",
    "        \n",
    "        # Decode base64 image\n",
    "        image = decode_base64_image(base64_image)\n",
    "        if image is None:\n",
    "            return jsonify({\n",
    "                'error': 'Image decoding failed',\n",
    "                'message': 'Unable to decode the provided base64 image. Please ensure it\\\\'s a valid image format.'\n",
    "            }), 400\n",
    "        \n",
    "        # Convert BGR to RGB for MediaPipe\n",
    "        rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Run hand detection\n",
    "        try:\n",
    "            logger.info(\"Running MediaPipe hand detection for prediction...\")\n",
    "            results = hands_model.process(rgb_image)\n",
    "            logger.info(f\"Detection complete. Found {len(results.multi_hand_landmarks) if results.multi_hand_landmarks else 0} hand(s)\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"MediaPipe detection error: {e}\")\n",
    "            return jsonify({\n",
    "                'error': 'Detection failed',\n",
    "                'message': 'An error occurred during hand detection processing.'\n",
    "            }), 500\n",
    "        \n",
    "        # Check if hands were detected\n",
    "        if not results.multi_hand_landmarks:\n",
    "            return jsonify({\n",
    "                'success': False,\n",
    "                'message': 'No hands detected in the image. Please ensure your hand is clearly visible.',\n",
    "                'predicted_gesture': None,\n",
    "                'confidence': 0.0,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "        \n",
    "        # Process landmarks\n",
    "        processed_hands = process_hand_landmarks(results)\n",
    "        \n",
    "        # Extract features for prediction (using first hand)\n",
    "        if not processed_hands:\n",
    "            return jsonify({\n",
    "                'success': False,\n",
    "                'message': 'Could not extract hand landmarks.',\n",
    "                'predicted_gesture': None,\n",
    "                'confidence': 0.0,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "        \n",
    "        first_hand = processed_hands[0]['landmarks']\n",
    "        if len(first_hand) != 21:\n",
    "            return jsonify({\n",
    "                'success': False,\n",
    "                'message': f'Expected 21 landmarks, got {len(first_hand)}.',\n",
    "                'predicted_gesture': None,\n",
    "                'confidence': 0.0,\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "        \n",
    "        # Prepare feature vector\n",
    "        features = []\n",
    "        for landmark in first_hand:\n",
    "            features.extend([landmark['x'], landmark['y'], landmark['z']])\n",
    "        \n",
    "        # Predict gesture\n",
    "        prediction = model.predict([features])[0]\n",
    "        prediction_proba = model.predict_proba([features])[0]\n",
    "        confidence = max(prediction_proba)\n",
    "        \n",
    "        # Get class probabilities\n",
    "        classes = model.classes_\n",
    "        probabilities = dict(zip(classes, prediction_proba))\n",
    "        \n",
    "        logger.info(f\"✅ Predicted gesture: {prediction} (confidence: {confidence:.4f})\")\n",
    "        \n",
    "        return jsonify({\n",
    "            'success': True,\n",
    "            'predicted_gesture': prediction,\n",
    "            'confidence': round(confidence, 4),\n",
    "            'all_probabilities': {k: round(v, 4) for k, v in probabilities.items()},\n",
    "            'handsDetected': len(processed_hands),\n",
    "            'handedness': processed_hands[0]['handedness'],\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error in detect_trained_gesture: {e}\")\n",
    "        return jsonify({\n",
    "            'error': 'Internal server error',\n",
    "            'message': 'An unexpected error occurred during gesture detection.',\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }), 500\n",
    "\n",
    "@app.route('/dataset-info', methods=['GET'])\n",
    "def dataset_info():\n",
    "    \"\"\"Get information about the recorded dataset.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(CSV_FILE_PATH):\n",
    "            return jsonify({\n",
    "                'exists': False,\n",
    "                'message': 'No dataset found. Start recording gestures first.',\n",
    "                'totalSamples': 0,\n",
    "                'gestures': {},\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "        \n",
    "        df = pd.read_csv(CSV_FILE_PATH)\n",
    "        gesture_counts = df['gesture_name'].value_counts()\n",
    "        \n",
    "        return jsonify({\n",
    "            'exists': True,\n",
    "            'totalSamples': len(df),\n",
    "            'gestureCount': len(gesture_counts),\n",
    "            'gestures': gesture_counts.to_dict(),\n",
    "            'filePath': CSV_FILE_PATH,\n",
    "            'modelExists': os.path.exists(MODEL_FILE_PATH),\n",
    "            'modelPath': MODEL_FILE_PATH if os.path.exists(MODEL_FILE_PATH) else None,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error getting dataset info: {e}\")\n",
    "        return jsonify({\n",
    "            'error': 'Internal server error',\n",
    "            'message': 'Could not retrieve dataset information.',\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }), 500\n",
    "\n",
    "print(\"✅ API endpoints part 2 defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9427bc65",
   "metadata": {},
   "source": [
    "## 9. Error Handling\n",
    "\n",
    "Comprehensive error handlers for different HTTP status codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e84d6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.errorhandler(413)\n",
    "def file_too_large(error):\n",
    "    \"\"\"Handle file too large errors.\"\"\"\n",
    "    return jsonify({\n",
    "        'error': 'File too large',\n",
    "        'message': 'The uploaded image is too large. Maximum size is 50MB.',\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }), 413\n",
    "\n",
    "@app.errorhandler(404)\n",
    "def not_found(error):\n",
    "    \"\"\"Handle 404 errors.\"\"\"\n",
    "    return jsonify({\n",
    "        'error': 'Not found',\n",
    "        'message': 'The requested endpoint was not found.',\n",
    "        'availableEndpoints': [\n",
    "            'GET /health',\n",
    "            'GET /model-info',\n",
    "            'GET /dataset-info',\n",
    "            'POST /record-gesture',\n",
    "            'POST /train',\n",
    "            'POST /detect'\n",
    "        ],\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }), 404\n",
    "\n",
    "@app.errorhandler(500)\n",
    "def internal_server_error(error):\n",
    "    \"\"\"Handle 500 errors.\"\"\"\n",
    "    logger.error(f\"Internal server error: {error}\")\n",
    "    return jsonify({\n",
    "        'error': 'Internal server error',\n",
    "        'message': 'An unexpected error occurred on the server.',\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }), 500\n",
    "\n",
    "print(\"✅ Error handlers defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be26d98b",
   "metadata": {},
   "source": [
    "## 10. Server Configuration and Startup\n",
    "\n",
    "Main function to configure and start the Flask server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6ee59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to start the Flask server.\"\"\"\n",
    "    print(\"🚀 ======================================\")\n",
    "    print(\"🚀 Hand Gesture Recognition API Server\")\n",
    "    print(\"🚀 Backend: Flask + MediaPipe\")\n",
    "    print(\"🚀 ======================================\")\n",
    "    \n",
    "    # Get configuration\n",
    "    host = os.environ.get('HOST', '127.0.0.1')\n",
    "    port = int(os.environ.get('PORT', 5000))\n",
    "    debug = os.environ.get('FLASK_DEBUG', 'False').lower() == 'true'\n",
    "    \n",
    "    print(f\"🚀 Host: {host}\")\n",
    "    print(f\"🚀 Port: {port}\")\n",
    "    print(f\"🚀 Debug: {debug}\")\n",
    "    print(f\"🚀 Health Check: http://{host}:{port}/health\")\n",
    "    print(f\"🚀 Model Info: http://{host}:{port}/model-info\")\n",
    "    print(f\"🚀 Dataset Info: http://{host}:{port}/dataset-info\")\n",
    "    print(f\"🚀 Record Gesture: POST http://{host}:{port}/record-gesture\")\n",
    "    print(f\"🚀 Train Model: POST http://{host}:{port}/train\")\n",
    "    print(f\"🚀 Detect Gesture: POST http://{host}:{port}/detect\")\n",
    "    print(\"🚀 ======================================\")\n",
    "    \n",
    "    try:\n",
    "        app.run(host=host, port=port, debug=debug)\n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Server shutdown requested by user\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Server error: {e}\")\n",
    "        return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "# Uncomment the line below to start the server\n",
    "# main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4405e0f2",
   "metadata": {},
   "source": [
    "## Testing the API\n",
    "\n",
    "Let's test our gesture recognition API endpoints to ensure everything works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9dfba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Test configuration\n",
    "API_BASE_URL = \"http://127.0.0.1:5000\"\n",
    "\n",
    "def test_health_endpoint():\n",
    "    \"\"\"Test the health check endpoint\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{API_BASE_URL}/health\")\n",
    "        print(\"🔍 Health Check Test:\")\n",
    "        print(f\"Status Code: {response.status_code}\")\n",
    "        print(f\"Response: {json.dumps(response.json(), indent=2)}\")\n",
    "        return response.status_code == 200\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Health check failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def test_model_info_endpoint():\n",
    "    \"\"\"Test the model info endpoint\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{API_BASE_URL}/model-info\")\n",
    "        print(\"\\n🔍 Model Info Test:\")\n",
    "        print(f\"Status Code: {response.status_code}\")\n",
    "        print(f\"Response: {json.dumps(response.json(), indent=2)}\")\n",
    "        return response.status_code == 200\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Model info test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def test_dataset_info_endpoint():\n",
    "    \"\"\"Test the dataset info endpoint\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{API_BASE_URL}/dataset-info\")\n",
    "        print(\"\\n🔍 Dataset Info Test:\")\n",
    "        print(f\"Status Code: {response.status_code}\")\n",
    "        print(f\"Response: {json.dumps(response.json(), indent=2)}\")\n",
    "        return response.status_code == 200\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Dataset info test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Note: To test record-gesture, train, and detect endpoints, you would need:\n",
    "# 1. Base64 encoded images of hand gestures\n",
    "# 2. A running Flask server\n",
    "# 3. Proper gesture samples for training\n",
    "\n",
    "print(\"🧪 API Test Functions Defined!\")\n",
    "print(\"📝 To run tests:\")\n",
    "print(\"   1. Start the Flask server: main()\")\n",
    "print(\"   2. Run: test_health_endpoint()\")\n",
    "print(\"   3. Run: test_model_info_endpoint()\")\n",
    "print(\"   4. Run: test_dataset_info_endpoint()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469bc952",
   "metadata": {},
   "source": [
    "## Usage Examples\n",
    "\n",
    "Here are examples of how to use the API endpoints:\n",
    "\n",
    "### 1. Recording a Gesture\n",
    "```python\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "# Read an image file and convert to base64\n",
    "with open(\"gesture_image.jpg\", \"rb\") as image_file:\n",
    "    encoded_string = base64.b64encode(image_file.read()).decode()\n",
    "\n",
    "# Record gesture\n",
    "response = requests.post(\"http://127.0.0.1:5000/record-gesture\", json={\n",
    "    \"gesture_name\": \"thumbs_up\",\n",
    "    \"image\": f\"data:image/jpeg;base64,{encoded_string}\"\n",
    "})\n",
    "```\n",
    "\n",
    "### 2. Training the Model\n",
    "```python\n",
    "response = requests.post(\"http://127.0.0.1:5000/train\")\n",
    "print(response.json())\n",
    "```\n",
    "\n",
    "### 3. Detecting a Gesture\n",
    "```python\n",
    "# After training, detect gesture from new image\n",
    "response = requests.post(\"http://127.0.0.1:5000/detect\", json={\n",
    "    \"image\": f\"data:image/jpeg;base64,{encoded_string}\"\n",
    "})\n",
    "print(f\"Predicted gesture: {response.json()['predicted_gesture']}\")\n",
    "```\n",
    "\n",
    "### API Endpoints Summary:\n",
    "- **GET /health**: Check server status\n",
    "- **GET /model-info**: Get MediaPipe model information\n",
    "- **GET /dataset-info**: Get dataset statistics\n",
    "- **POST /record-gesture**: Record gesture landmarks to CSV\n",
    "- **POST /train**: Train RandomForest classifier\n",
    "- **POST /detect**: Predict gesture from image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be515523",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a complete implementation of a hand gesture recognition Flask API with the following features:\n",
    "\n",
    "### ✅ Requirements Fulfilled:\n",
    "\n",
    "1. **MediaPipe Hands Integration**: ✅ Extracts 21 hand landmarks from base64 images\n",
    "2. **Record Gesture Endpoint**: ✅ `/record-gesture` saves landmarks to CSV (gesture_name, x0,y0,z0, ..., x20,y20,z20)\n",
    "3. **Train Endpoint**: ✅ `/train` loads CSV data, trains RandomForest, saves model as `gesture_model.pkl`\n",
    "4. **Detect Endpoint**: ✅ `/detect` predicts gesture from base64 image using trained model\n",
    "5. **CORS Support**: ✅ Flask-CORS enabled for frontend communication\n",
    "\n",
    "### 🛠️ Key Features:\n",
    "\n",
    "- **Clean Architecture**: Modular functions, proper error handling, logging\n",
    "- **Production Ready**: Comprehensive validation, error responses, health checks\n",
    "- **Extensible**: Easy to add new gestures, modify ML parameters\n",
    "- **Developer Friendly**: Clear documentation, type hints, detailed comments\n",
    "\n",
    "### 📁 Generated Files:\n",
    "- `gesture_data.csv`: Training data with hand landmarks\n",
    "- `gesture_model.pkl`: Trained RandomForest classifier\n",
    "\n",
    "### 🚀 Next Steps:\n",
    "1. Install dependencies: `pip install -r requirements.txt`\n",
    "2. Run the notebook cells to define all functions\n",
    "3. Start server: `main()`\n",
    "4. Test endpoints or integrate with frontend\n",
    "\n",
    "The code is clean, well-documented, and ready for production use or further development by other developers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e619bde",
   "metadata": {},
   "source": [
    "## 🚀 Complete Integration Testing\n",
    "\n",
    "### Step-by-Step Workflow Test:\n",
    "\n",
    "#### 1. Start the Backend Server\n",
    "```bash\n",
    "cd D:\\EPICS\\backend\n",
    "python app.py\n",
    "```\n",
    "\n",
    "#### 2. Start the Frontend Application\n",
    "```bash\n",
    "cd D:\\EPICS\\gesture-control-hub\n",
    "npm start\n",
    "```\n",
    "\n",
    "#### 3. Test the Complete ML Workflow\n",
    "\n",
    "**Phase 1: Record Training Data**\n",
    "1. Navigate to the \"📹 Record Gestures\" tab\n",
    "2. Record 5-10 samples each of different gestures:\n",
    "   - `thumbs_up` (👍)\n",
    "   - `thumbs_down` (👎) \n",
    "   - `open_hand` (✋)\n",
    "   - `fist` (👊)\n",
    "   - `peace_sign` (✌️)\n",
    "\n",
    "**Phase 2: Train the Model**\n",
    "1. Navigate to the \"🤖 Train Model\" tab\n",
    "2. Click \"🚀 Start Training\"\n",
    "3. Wait for training to complete\n",
    "4. Verify accuracy metrics\n",
    "\n",
    "**Phase 3: Test Real-Time Detection**\n",
    "1. Navigate to the \"🏠 Device Control\" tab\n",
    "2. Toggle \"Use ML Model\" to ON\n",
    "3. Test gestures in front of camera\n",
    "4. Verify device control responses:\n",
    "   - 👍 → Increase fan speed\n",
    "   - 👎 → Decrease fan speed\n",
    "   - ✋ → Turn off all lights\n",
    "   - Point finger → Toggle TV\n",
    "\n",
    "### Expected Results:\n",
    "- ✅ Backend serving on http://127.0.0.1:5000\n",
    "- ✅ Frontend serving on http://localhost:3000\n",
    "- ✅ Real-time gesture detection with 85%+ accuracy\n",
    "- ✅ Seamless device control integration\n",
    "- ✅ ML model outperforming landmark analysis\n",
    "\n",
    "### Troubleshooting:\n",
    "1. **CORS Issues**: Ensure Flask-CORS is properly configured\n",
    "2. **Camera Access**: Check browser permissions\n",
    "3. **Model Training**: Ensure minimum 3 samples per gesture\n",
    "4. **Backend Connection**: Verify Flask server is running on port 5000\n",
    "\n",
    "The system now provides a complete machine learning pipeline for custom gesture recognition! 🎉"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
